{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BA_climate  IECC_climate_code   HDD65   CDD65  HDD30YR_PUB  \\\n",
      "0             4.0                7.0  3844.0  1679.0       4451.0   \n",
      "1             5.0                6.0  3766.0  1458.0       4429.0   \n",
      "2             4.0                7.0  3819.0  1696.0       4500.0   \n",
      "3             5.0                3.0  2614.0  1718.0       3229.0   \n",
      "4             5.0                6.0  4219.0  1363.0       4896.0   \n",
      "...           ...                ...     ...     ...          ...   \n",
      "18491         5.0                6.0  4572.0  1037.0       4547.0   \n",
      "18492         5.5               13.0  9861.0   283.0       9862.0   \n",
      "18493         2.0                2.0   405.0  4725.0        672.0   \n",
      "18494         2.0                3.0  1245.0  3038.0       1752.0   \n",
      "18495         5.0                6.0  4423.0  1424.0       4225.0   \n",
      "\n",
      "       CDD30YR_PUB  TYPEHUQ  STORIES  BEDROOMS  NCOMBATH  ...  HHAGE  \\\n",
      "0           1027.0    2.000    2.000     4.000     3.000  ...   65.0   \n",
      "1           1305.0    3.000    2.000     3.125     2.125  ...   79.0   \n",
      "2           1010.0    3.625    2.000     2.375     2.000  ...   82.0   \n",
      "3           1653.0    2.000    2.000     2.000     2.000  ...   70.0   \n",
      "4           1059.0    3.000    2.000     2.000     2.125  ...   30.0   \n",
      "...            ...      ...      ...       ...       ...  ...    ...   \n",
      "18491       1190.0    3.000    2.000     3.000     2.000  ...   37.0   \n",
      "18492        186.0    2.000    2.375     3.000     2.375  ...   58.0   \n",
      "18493       4047.0    2.000    2.000     3.000     2.000  ...   25.0   \n",
      "18494       2295.0    2.000    2.000     3.000     3.000  ...   66.0   \n",
      "18495       1680.0    2.000    2.125     3.000     2.000  ...   68.0   \n",
      "\n",
      "       NHSLDMEM  NUMCHILD  ATHOME  MONEYPY  SQFTRANGE  TOTSQFT_EN  TOTHSQFT  \\\n",
      "0          2.00     2.125   2.875   13.000      6.000      2100.0    2100.0   \n",
      "1          2.75     2.250   2.625    6.000      2.500       590.0     590.0   \n",
      "2          2.50     2.375   3.000   11.000      3.000       900.0     900.0   \n",
      "3          2.00     2.125   3.250   10.000      6.000      2100.0    2100.0   \n",
      "4          2.00     2.250   2.625   12.125      3.000       800.0     800.0   \n",
      "...         ...       ...     ...      ...        ...         ...       ...   \n",
      "18491      4.00     2.000   2.875    2.000      4.000      1500.0    1500.0   \n",
      "18492      2.50     2.250   2.750   13.000      5.000      3070.0    3070.0   \n",
      "18493      4.00     2.500   3.000    8.000      5.000      1500.0    1120.0   \n",
      "18494      3.25     2.250   3.000   14.000      6.875      3000.0    3000.0   \n",
      "18495      2.00     2.000   3.000   14.000      6.000      2000.0    2000.0   \n",
      "\n",
      "       TOTCSQFT       KWH  \n",
      "0        2100.0  12521.48  \n",
      "1         590.0   5243.05  \n",
      "2         900.0   2387.64  \n",
      "3        2100.0   9275.07  \n",
      "4         800.0   5869.70  \n",
      "...         ...       ...  \n",
      "18491    1500.0   5638.33  \n",
      "18492    1530.0   4425.20  \n",
      "18493     900.0  15121.25  \n",
      "18494    3000.0  18604.35  \n",
      "18495    2000.0  19818.82  \n",
      "\n",
      "[18496 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of column names with numbers removed\n",
    "columns = [\n",
    "    \"BA_climate\", \"IECC_climate_code\", \"HDD65\", \"CDD65\", \"HDD30YR_PUB\", \"CDD30YR_PUB\", \n",
    "    \"TYPEHUQ\", \"STORIES\", \"BEDROOMS\", \"NCOMBATH\", \"OTHROOMS\", \"TOTROOMS\", \"WINDOWS\", \n",
    "    \"ADQINSUL\", \"NUMFRIG\", \"RCOOKUSE\", \"ROVENUSE\", \"NUMMEAL\", \"DWASHUSE\", \"WASHLOAD\", \n",
    "    \"DRYRUSE\", \"EQUIPM\", \"NUMPORTEL\", \"NUMPORTHUM\", \"ACEQUIPM_PUB\", \"NUMPORTAC\", \n",
    "    \"NUMCFAN\", \"NUMFLOORFAN\", \"USECFAN\", \"LGTIN1TO4\", \"LGTIN4TO8\", \"LGTINMORE8\", \"HHAGE\", \n",
    "    \"NHSLDMEM\", \"NUMCHILD\", \"ATHOME\", \"MONEYPY\", \"SQFTRANGE\", \"TOTSQFT_EN\", \"TOTHSQFT\", \n",
    "    \"TOTCSQFT\", \"KWH\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the column names\n",
    "df = pd.read_csv(\"Final_data.csv\")\n",
    "df = df[columns]\n",
    "# Display the empty DataFrame with the specified columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"KWH\",axis=1)\n",
    "y = df[\"KWH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Mean Squared Error of the stacked model: 12498883.646801984\n",
      "Mean Absolute Error of the stacked model: 2523.7893684792107\n",
      "R-squared of the stacked model: 0.6694262554775948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [20:17:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['meta_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default model we plan to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = model.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = model.predict(X_test)\n",
    "\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), gb_preds_train,xgb_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), gb_preds_test,xgb_preds_test))\n",
    "\n",
    "# Step 7: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Step 8: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n",
    "joblib.dump(rf, \"random_forest.pkl\")\n",
    "joblib.dump(model, \"neural_network.h5\")\n",
    "joblib.dump(gb, \"gradient_boosting.pkl\")\n",
    "xgb_model.save_model('xgboost_model.pkl')\n",
    "joblib.dump(meta_model, 'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using knn model (k nearest neighbors)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = model.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = model.predict(X_test)\n",
    "'''\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "'''\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Train the K-Nearest Neighbors model\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds_train = knn.predict(X_train)\n",
    "knn_preds_test = knn.predict(X_test)\n",
    "\n",
    "# Step 7: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), xgb_preds_train, knn_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), xgb_preds_test, knn_preds_test))\n",
    "\n",
    "# Step 8: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Step 9: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using knn and svm (support vector machine) model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = model.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = model.predict(X_test)\n",
    "\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Train the K-Nearest Neighbors model\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds_train = knn.predict(X_train)\n",
    "knn_preds_test = knn.predict(X_test)\n",
    "\n",
    "# Step 7: Train the Support Vector Machine (SVM) model\n",
    "svm = SVR(kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_preds_train = svm.predict(X_train)\n",
    "svm_preds_test = svm.predict(X_test)\n",
    "\n",
    "# Step 8: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), gb_preds_train, xgb_preds_train, knn_preds_train, svm_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), gb_preds_test, xgb_preds_test, knn_preds_test, svm_preds_test))\n",
    "\n",
    "# Step 9: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Step 10: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Step 11: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
