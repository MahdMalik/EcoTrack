{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BA_climate  IECC_climate_code   HDD65   CDD65  HDD30YR_PUB  \\\n",
      "0             4.0                7.0  3844.0  1679.0       4451.0   \n",
      "1             5.0                6.0  3766.0  1458.0       4429.0   \n",
      "2             4.0                7.0  3819.0  1696.0       4500.0   \n",
      "3             5.0                3.0  2614.0  1718.0       3229.0   \n",
      "4             5.0                6.0  4219.0  1363.0       4896.0   \n",
      "...           ...                ...     ...     ...          ...   \n",
      "18491         5.0                6.0  4572.0  1037.0       4547.0   \n",
      "18492         5.5               13.0  9861.0   283.0       9862.0   \n",
      "18493         2.0                2.0   405.0  4725.0        672.0   \n",
      "18494         2.0                3.0  1245.0  3038.0       1752.0   \n",
      "18495         5.0                6.0  4423.0  1424.0       4225.0   \n",
      "\n",
      "       CDD30YR_PUB  TYPEHUQ  STORIES  BEDROOMS  NCOMBATH  ...  HHAGE  \\\n",
      "0           1027.0    2.000    2.000     4.000     3.000  ...   65.0   \n",
      "1           1305.0    3.000    2.000     3.125     2.125  ...   79.0   \n",
      "2           1010.0    3.625    2.000     2.375     2.000  ...   82.0   \n",
      "3           1653.0    2.000    2.000     2.000     2.000  ...   70.0   \n",
      "4           1059.0    3.000    2.000     2.000     2.125  ...   30.0   \n",
      "...            ...      ...      ...       ...       ...  ...    ...   \n",
      "18491       1190.0    3.000    2.000     3.000     2.000  ...   37.0   \n",
      "18492        186.0    2.000    2.375     3.000     2.375  ...   58.0   \n",
      "18493       4047.0    2.000    2.000     3.000     2.000  ...   25.0   \n",
      "18494       2295.0    2.000    2.000     3.000     3.000  ...   66.0   \n",
      "18495       1680.0    2.000    2.125     3.000     2.000  ...   68.0   \n",
      "\n",
      "       NHSLDMEM  NUMCHILD  ATHOME  MONEYPY  SQFTRANGE  TOTSQFT_EN  TOTHSQFT  \\\n",
      "0          2.00     2.125   2.875   13.000      6.000      2100.0    2100.0   \n",
      "1          2.75     2.250   2.625    6.000      2.500       590.0     590.0   \n",
      "2          2.50     2.375   3.000   11.000      3.000       900.0     900.0   \n",
      "3          2.00     2.125   3.250   10.000      6.000      2100.0    2100.0   \n",
      "4          2.00     2.250   2.625   12.125      3.000       800.0     800.0   \n",
      "...         ...       ...     ...      ...        ...         ...       ...   \n",
      "18491      4.00     2.000   2.875    2.000      4.000      1500.0    1500.0   \n",
      "18492      2.50     2.250   2.750   13.000      5.000      3070.0    3070.0   \n",
      "18493      4.00     2.500   3.000    8.000      5.000      1500.0    1120.0   \n",
      "18494      3.25     2.250   3.000   14.000      6.875      3000.0    3000.0   \n",
      "18495      2.00     2.000   3.000   14.000      6.000      2000.0    2000.0   \n",
      "\n",
      "       TOTCSQFT       KWH  \n",
      "0        2100.0  12521.48  \n",
      "1         590.0   5243.05  \n",
      "2         900.0   2387.64  \n",
      "3        2100.0   9275.07  \n",
      "4         800.0   5869.70  \n",
      "...         ...       ...  \n",
      "18491    1500.0   5638.33  \n",
      "18492    1530.0   4425.20  \n",
      "18493     900.0  15121.25  \n",
      "18494    3000.0  18604.35  \n",
      "18495    2000.0  19818.82  \n",
      "\n",
      "[18496 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of column names with numbers removed\n",
    "columns = [\n",
    "    \"BA_climate\", \"IECC_climate_code\", \"HDD65\", \"CDD65\", \"HDD30YR_PUB\", \"CDD30YR_PUB\", \n",
    "    \"TYPEHUQ\", \"STORIES\", \"BEDROOMS\", \"NCOMBATH\", \"OTHROOMS\", \"TOTROOMS\", \"WINDOWS\", \n",
    "    \"ADQINSUL\", \"NUMFRIG\", \"RCOOKUSE\", \"ROVENUSE\", \"NUMMEAL\", \"DWASHUSE\", \"WASHLOAD\", \n",
    "    \"DRYRUSE\", \"EQUIPM\", \"NUMPORTEL\", \"NUMPORTHUM\", \"ACEQUIPM_PUB\", \"NUMPORTAC\", \n",
    "    \"NUMCFAN\", \"NUMFLOORFAN\", \"USECFAN\", \"LGTIN1TO4\", \"LGTIN4TO8\", \"LGTINMORE8\", \"HHAGE\", \n",
    "    \"NHSLDMEM\", \"NUMCHILD\", \"ATHOME\", \"MONEYPY\", \"SQFTRANGE\", \"TOTSQFT_EN\", \"TOTHSQFT\", \n",
    "    \"TOTCSQFT\", \"KWH\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the column names\n",
    "df = pd.read_csv(\"Final_data.csv\")\n",
    "df = df[columns]\n",
    "# Display the empty DataFrame with the specified columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"KWH\",axis=1)\n",
    "y = df[\"KWH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# default model we plan to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "nn.add(Dropout(0.3))\n",
    "nn.add(Dense(units=128, activation='relu'))\n",
    "nn.add(Dropout(0.3))\n",
    "nn.add(Dense(units=128, activation='relu'))\n",
    "nn.add(Dense(units=128, activation='relu'))\n",
    "nn.add(Dense(units=32, activation='relu'))\n",
    "nn.add(Dense(1, activation='linear'))  # Output layer\n",
    "nn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "nn.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = nn.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = nn.predict(X_test)\n",
    "\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), gb_preds_train,xgb_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), gb_preds_test,xgb_preds_test))\n",
    "\n",
    "# Step 7: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# # Step 8: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# # # Step 9: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n",
    "joblib.dump(rf, \"random_forest.pkl\")\n",
    "nn.save(\"neural_network.h5\")\n",
    "joblib.dump(gb, \"gradient_boosting.pkl\")\n",
    "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n",
    "joblib.dump(meta_model, 'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step\n",
      "Mean Squared Error of the stacked model: 12637104.098853076\n",
      "Mean Absolute Error of the stacked model: 2547.599091374098\n",
      "R-squared of the stacked model: 0.6657705648019081\n"
     ]
    }
   ],
   "source": [
    "# model using knn model (k nearest neighbors)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = model.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = model.predict(X_test)\n",
    "'''\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "'''\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Train the K-Nearest Neighbors model\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds_train = knn.predict(X_train)\n",
    "knn_preds_test = knn.predict(X_test)\n",
    "\n",
    "# Step 7: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), xgb_preds_train, knn_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), xgb_preds_test, knn_preds_test))\n",
    "\n",
    "# Step 8: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Step 9: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Step 3: Get predictions from both models\u001b[39;00m\n\u001b[0;32m     41\u001b[0m rf_preds_train \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model using knn and svm (support vector machine) model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Step 1: Train the Random Forest Regressor with the best parameters\n",
    "rf = RandomForestRegressor(n_estimators=300, \n",
    "                           min_samples_split=5, \n",
    "                           min_samples_leaf=1, \n",
    "                           max_features='sqrt', \n",
    "                           max_depth=30, \n",
    "                           random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Train the Neural Network with the given architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))  # First hidden layer\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "rf_preds_train = rf.predict(X_train)\n",
    "nn_preds_train = model.predict(X_train)\n",
    "\n",
    "rf_preds_test = rf.predict(X_test)\n",
    "nn_preds_test = model.predict(X_test)\n",
    "\n",
    "# Step 4: Train the Gradient Boosting Regressor with the given parameters\n",
    "gb = GradientBoostingRegressor(n_estimators=500,  # Updated to match the best parameter\n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=6, \n",
    "                               subsample=1.0,  # Use 100% of the samples\n",
    "                               random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb_preds_train = gb.predict(X_train)\n",
    "gb_preds_test = gb.predict(X_test)\n",
    "\n",
    "# Step 5: Train XGBoost with the provided parameters\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.9, \n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=500, \n",
    "                             subsample=1.0, \n",
    "                             random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds_train = xgb_model.predict(X_train)\n",
    "xgb_preds_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 6: Train the K-Nearest Neighbors model\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds_train = knn.predict(X_train)\n",
    "knn_preds_test = knn.predict(X_test)\n",
    "\n",
    "# Step 7: Train the Support Vector Machine (SVM) model\n",
    "svm = SVR(kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_preds_train = svm.predict(X_train)\n",
    "svm_preds_test = svm.predict(X_test)\n",
    "\n",
    "# Step 8: Combine predictions from all models\n",
    "X_meta_train = np.column_stack((rf_preds_train, nn_preds_train.flatten(), gb_preds_train, xgb_preds_train, knn_preds_train, svm_preds_train))\n",
    "X_meta_test = np.column_stack((rf_preds_test, nn_preds_test.flatten(), gb_preds_test, xgb_preds_test, knn_preds_test, svm_preds_test))\n",
    "\n",
    "# Step 9: Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Step 10: Make final predictions with the meta-model\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Step 11: Evaluate the model\n",
    "mse = mean_squared_error(y_test, final_preds)\n",
    "print(f'Mean Squared Error of the stacked model: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_preds)\n",
    "r2 = r2_score(y_test, final_preds)\n",
    "print(f'Mean Absolute Error of the stacked model: {mae}')\n",
    "print(f'R-squared of the stacked model: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
